# Improved DQN Training Configuration for Cracer Simulation
# This config uses Double DQN + Dueling Architecture + Prioritized Experience Replay

# Training duration
total_episodes: 5000
max_steps_per_episode: 10000

# Replay buffer
memory_size: 100000
batch_size: 64
min_replay_size: 1000

# DQN hyperparameters
gamma: 0.99
learning_rate: 0.0001
lr_decay: 0.9999
lr_min: 0.000001

# Exploration - faster decay for quicker convergence
eps_start: 1.0
eps_end: 0.02
eps_decay: 50000.0

# Target network - softer updates
tau: 0.001
target_update_freq: 1

# Network architecture
hidden_sizes:
  - 512
  - 256
  - 128
use_dueling: true
use_double: true
use_layer_norm: true

# Prioritized Experience Replay
use_per: true
per_alpha: 0.6
per_beta_start: 0.4
per_beta_end: 1.0
per_beta_episodes: 4000
per_epsilon: 0.000001

# Reward shaping - balanced values to avoid extreme risk aversion
reward_speed_scale: 0.5
reward_fuel_bonus: 100.0
reward_crash_penalty: 200.0
reward_pothole_penalty: 20.0
reward_bump_penalty: 10.0
reward_survival_bonus: 0.5
reward_distance_scale: 1.0
reward_stage_clear_bonus: 500.0

# Low fuel urgency
low_fuel_penalty_scale: 2.0
low_fuel_threshold: 0.3

# Fuel direction guidance
fuel_direction_scale: 5.0

# Episode termination - let agent learn recovery from crashes
terminate_on_crash: false

# Reward normalization for stable training
normalize_rewards: true
reward_clip: 10.0

# Gradient clipping
grad_clip_norm: 10.0

# Checkpointing
save_every: 100
save_dir: rl/checkpoints
log_interval: 10

# Environment
seed: 42
device: auto
resume: ""
render: false
